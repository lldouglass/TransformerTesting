# TransformerTesting
# Look for transformer in transformers-yo.ipynb
# Additional transformer in newtransformer.ipynb

## William Warner

##### Work Done

**-** Implemented the function that makes the mask matrix <br>
**-** Created the forward pass function with main Transformer equation<br>
**-** Added embeddings and unembeddings <br>
**-** Added softmax <br>
**-** Added transformer output visualization <br>

##### ChatGPT Logs

**-** https://chatgpt.com/share/67d3947b-1b30-800c-b7a1-bfc754638a9e <br>
**-** https://chatgpt.com/share/67d394a5-8614-800c-bf3a-5a2626557c32 <br>


###Logan Douglass 

#####Work Done 

**-**  
**-** I developed a tokenization pipeline and built our embeddings matrix to efficiently convert raw text into meaningful vectors.
I implemented a reference version of the GPT-2 tokenizer.
Implimented the transformer architecture by  adding self-attention, positional encoding, and feed-forward layers. 
Added a function to parse the data from the guetenburg library.
Also supported a lot of the team members with git.
##### ChatGPT Logs
https://chatgpt.com/share/67d429ca-a484-800d-8600-48324a1ae18f
https://chatgpt.com/share/67d429e4-e7a4-800d-bd50-37df707c801d
https://chatgpt.com/share/67d429fa-9a6c-800d-82e1-6f47c67abb83
https://chatgpt.com/share/67d42a1f-ae64-800d-93f5-0e33eb1c53d0
I used chatgpt for basic understanding of the transformer architecture. I also used it for a lot of the code implimentation and debugging code that I wrote. 
I used github copilot for information about the virtual enviornment, debugging and the embeddings. 

## Henry Finnila

##### Work Done

**-** Debugged Training loop and some areas of MLP to get training loop working correctly and input correct loss
**-** Added RMS norm to our transformer
**-** Added positional embeddings
**-** Added GELU

##### ChatGPT Logs

**-** [ChatGPTConvo1.pdf](https://github.com/user-attachments/files/19257780/ChatGPTConvo1.pdf)

**-** [https://chatgpt.com/share/67d4f6a2-3180-800b-adbe-d7d3debbb148](url)


## Elias Shiffman

##### Work Done

**-** Implemented attention head, multi-headed attention, and mlp classes
**-** Helped with parsing book data into usable batchs
**-** Added positional embeddings, layernorm, and GELU
**-** Wrote code to generate tokens for the trained model

##### ChatGPT Logs

**-** https://chatgpt.com/share/67d5ebda-e6cc-800a-b243-0c8bf52b6f89 <br>

**-** https://chatgpt.com/share/67d5ec6d-a940-800a-8478-32f56fb306ef <br>

## Kasra Ghahremani
##### Work Done

**-** Helped with Implementation of the function that makes the mask matrix <br>
**-** Helped with softmax
**-** Helped with parsing book data into usable batchs

