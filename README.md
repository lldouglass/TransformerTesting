# TransformerTesting


## William Warner

##### Work Done

**-** Implemented the function that makes the mask matrix <br>
**-** Created the forward pass function with main Transformer equation<br>
**-** Added embeddings and unembeddings <br>
**-** Added softmax <br>
**-** Added transformer output visualization <br>

##### ChatGPT Logs

**-** https://chatgpt.com/share/67d3947b-1b30-800c-b7a1-bfc754638a9e <br>
**-** https://chatgpt.com/share/67d394a5-8614-800c-bf3a-5a2626557c32 <br>


###Logan Douglass 

#####Work Done 

**-**  
**-** I developed a tokenization pipeline and built our embeddings matrix to efficiently convert raw text into meaningful vectors.
I implemented a reference version of the GPT-2 tokenizer, ensuring our tokenization process was both accurate and aligned with industry standards.
I provided a clear example of the transformer architecture by detailing key components like self-attention, positional encoding, and feed-forward layers.
I offered comprehensive documentation and hands-on support, helping the team overcome implementation challenges and optimize the modelâ€™s performance.
##### ChatGPT Logs
https://chatgpt.com/share/67d429ca-a484-800d-8600-48324a1ae18f
https://chatgpt.com/share/67d429e4-e7a4-800d-bd50-37df707c801d
https://chatgpt.com/share/67d429fa-9a6c-800d-82e1-6f47c67abb83
https://chatgpt.com/share/67d42a1f-ae64-800d-93f5-0e33eb1c53d0

## Henry Finnila

##### Work Done

**-** Debugged Training loop and some areas of MLP to get training loop working correctly and input correct loss
**-** Added RMS norm to our transformer
**-** Added positional embeddings
**-** Added GELU

##### ChatGPT Logs

**-** [ChatGPTConvo1.pdf](https://github.com/user-attachments/files/19257780/ChatGPTConvo1.pdf)

**-** [https://chatgpt.com/share/67d4f6a2-3180-800b-adbe-d7d3debbb148](url)
