{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "\"\"\"\n",
    "transformer_rmsnorm_gelu.py\n",
    "\n",
    "Single Python script that:\n",
    "  1) Implements a Transformer with:\n",
    "     - Learned positional embeddings\n",
    "     - RMSNorm layers\n",
    "     - GELU activation in the MLP\n",
    "  2) Downloads Gutenberg books\n",
    "  3) Tokenizes data and trains the Transformer\n",
    "\"\"\"\n",
    "\n",
    "import math\n",
    "import requests\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "###############################################################################\n",
    "# RMSNorm Implementation\n",
    "###############################################################################\n",
    "\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, d_model: int, eps: float = 1e-8):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(d_model))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute RMS norm over the last dimension\n",
    "        # x: [n_context, d_model]\n",
    "        norm = x.pow(2).mean(-1, keepdim=True).sqrt()  # RMS across last dimension\n",
    "        return self.weight * x / (norm + self.eps)\n",
    "\n",
    "###############################################################################\n",
    "# Causal Mask Creation\n",
    "###############################################################################\n",
    "\n",
    "def create_mask(n_context: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates an upper-triangular mask (causal mask) of shape (n_context, n_context)\n",
    "    where positions cannot attend to subsequent positions.\n",
    "    \"\"\"\n",
    "    mask = torch.zeros(n_context, n_context)\n",
    "    # indices above the main diagonal\n",
    "    indices = torch.triu_indices(n_context, n_context, offset=1)\n",
    "    mask[indices[0], indices[1]] = float('-inf')\n",
    "    return mask\n",
    "\n",
    "# Optional: visualize the mask (comment out if not needed)\n",
    "if __name__ == \"__main__\":\n",
    "    M = create_mask(100)\n",
    "    plt.imshow(M.numpy(), cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title('Torch Matrix Visualization')\n",
    "    plt.show()\n",
    "\n",
    "###############################################################################\n",
    "# Configuration Dataclass\n",
    "###############################################################################\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    d_vocab: int = 10_000\n",
    "    d_model: int = 128\n",
    "    d_mlp: int = 512\n",
    "    n_heads: int = 4\n",
    "    d_head: int = 32\n",
    "    n_layers: int = 6\n",
    "    # Use GELU here\n",
    "    act_fn: type = nn.GELU  \n",
    "    n_ctx: int = 512  # maximum sequence length for positional embeddings\n",
    "\n",
    "###############################################################################\n",
    "# AttentionHead\n",
    "###############################################################################\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Single attention head for the Transformer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "        self.W_K = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "        self.W_V = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "        self.W_O = nn.Linear(cfg.d_head, cfg.d_model)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [n_context, d_model]\n",
    "        Returns: [n_context, d_model]\n",
    "        \"\"\"\n",
    "        n_context = x.size(0)\n",
    "        Q = self.W_Q(x)  # [n_context, d_head]\n",
    "        K = self.W_K(x)  # [n_context, d_head]\n",
    "        V = self.W_V(x)  # [n_context, d_head]\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        M = create_mask(n_context).to(x.device)  # [n_context, n_context]\n",
    "        scores = (Q @ K.T) / math.sqrt(Q.size(-1))  # [n_context, n_context]\n",
    "        scores = scores + M\n",
    "        A = F.softmax(scores, dim=-1)  # [n_context, n_context]\n",
    "        head_output = A @ V  # [n_context, d_head]\n",
    "\n",
    "        return self.W_O(head_output)  # [n_context, d_model]\n",
    "\n",
    "###############################################################################\n",
    "# MultiHeadedAttention\n",
    "###############################################################################\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Applies multiple attention heads and sums their outputs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.heads = nn.ModuleList([AttentionHead(cfg) for _ in range(cfg.n_heads)])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [n_context, d_model]\n",
    "        Returns: [n_context, d_model]\n",
    "        \"\"\"\n",
    "        out = 0\n",
    "        for head in self.heads:\n",
    "            out = out + head(x)\n",
    "        return out\n",
    "\n",
    "###############################################################################\n",
    "# MLP\n",
    "###############################################################################\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Feed-forward block for each Transformer layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
    "        self.output = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
    "        # Use the activation function specified in cfg (now GELU)\n",
    "        self.act = cfg.act_fn()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [n_context, d_model]\n",
    "        Returns: [n_context, d_model]\n",
    "        \"\"\"\n",
    "        return self.output(self.act(self.hidden(x)))\n",
    "\n",
    "###############################################################################\n",
    "# Transformer\n",
    "###############################################################################\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple GPT-style Transformer with:\n",
    "    - Learned token embeddings\n",
    "    - Learned positional embeddings\n",
    "    - RMSNorm\n",
    "    - GELU in the MLP\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg: GPTConfig):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "\n",
    "        # Token embedding\n",
    "        self.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\n",
    "        # Learned positional embeddings\n",
    "        self.pos_embedding = nn.Embedding(cfg.n_ctx, cfg.d_model)\n",
    "\n",
    "        # Unembedding layer\n",
    "        self.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)\n",
    "\n",
    "        # Transformer layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.ModuleDict({\n",
    "                'attn': MultiHeadedAttention(cfg),\n",
    "                'norm1': RMSNorm(cfg.d_model),\n",
    "                'mlp': MLP(cfg),\n",
    "                'norm2': RMSNorm(cfg.d_model)\n",
    "            })\n",
    "            for _ in range(cfg.n_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: [n_context] (token IDs)\n",
    "        Returns: [n_context, d_vocab] (logits)\n",
    "        \"\"\"\n",
    "        # Token embeddings\n",
    "        x = self.embedding(x)  # [n_context, d_model]\n",
    "\n",
    "        # Positional embeddings\n",
    "        positions = torch.arange(x.size(0), device=x.device)\n",
    "        pos_embeds = self.pos_embedding(positions)\n",
    "        x = x + pos_embeds\n",
    "\n",
    "        # Transformer layers\n",
    "        for layer in self.layers:\n",
    "            # Multi-head attention\n",
    "            x = x + layer['attn'](x)\n",
    "            x = layer['norm1'](x)\n",
    "\n",
    "            # MLP\n",
    "            x = x + layer['mlp'](x)\n",
    "            x = layer['norm2'](x)\n",
    "\n",
    "        # Final unembedding to produce logits\n",
    "        return self.unembedding(x)\n",
    "\n",
    "###############################################################################\n",
    "# Gutenberg Book Utilities (get_gutenberg_book, get_many_books)\n",
    "###############################################################################\n",
    "\n",
    "def get_gutenberg_book(\n",
    "    id: int = 84,\n",
    "    data_temp: Path | str = \"./data/gutenberg_data\",\n",
    "    remove_gutenberg_meta: bool = True\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Download a book from Project Gutenberg or load from cache.\n",
    "    \"\"\"\n",
    "    data_temp = Path(data_temp)\n",
    "    data_temp.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    url: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "    data_path: Path = data_temp / f\"{id}.txt\"\n",
    "\n",
    "    if data_path.exists():\n",
    "        with open(data_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = file.read()\n",
    "    else:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        data = response.text\n",
    "        with open(data_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(data)\n",
    "\n",
    "    if remove_gutenberg_meta:\n",
    "        # remove header/footer\n",
    "        data = '***'.join(data.split('***')[2:])\n",
    "        data = '***'.join(data.split('***')[:-1])\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_many_books(\n",
    "    ids: list[int],\n",
    "    data_temp: Path | str = \"./data/gutenberg_data\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Download multiple Gutenberg books or load from cache.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for book_id in ids:\n",
    "        print(f\"Getting book {book_id}...\")\n",
    "        text = get_gutenberg_book(book_id, data_temp)\n",
    "        print(f\"\\t{len(text)} characters read\")\n",
    "        data.append(text)\n",
    "    return data\n",
    "\n",
    "###############################################################################\n",
    "# Main / Example Usage\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # 1. Test basic Transformer shape\n",
    "    cfg = GPTConfig()\n",
    "    transform = Transformer(cfg)\n",
    "\n",
    "    n_context = 10\n",
    "    x = torch.randint(0, cfg.d_vocab, (n_context,))\n",
    "    print(\"Input shape:\", x.shape)\n",
    "    output = transform(x)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "    # Optional: visualize a subset of output\n",
    "    plt.imshow(output.detach().numpy()[:, :100], cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title(\"Transformer Output Visualization (subset of logits)\")\n",
    "    plt.xlabel(\"Vocab dimension (partial)\")\n",
    "    plt.ylabel(\"Context positions\")\n",
    "    plt.show()\n",
    "\n",
    "    # 2. Test AttentionHead separately\n",
    "    ah = AttentionHead(cfg)\n",
    "    test_x = torch.randn(10, cfg.d_model)\n",
    "    print(\"Attention head output shape:\", ah(test_x).shape)\n",
    "\n",
    "    attn_out = ah(test_x)\n",
    "    plt.imshow(attn_out.detach().numpy(), cmap='viridis', interpolation='nearest')\n",
    "    plt.colorbar(label='Value')\n",
    "    plt.title(\"AttentionHead Output Visualization\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Context positions\")\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Training Setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    model_cfg = GPTConfig(\n",
    "        d_model=128,\n",
    "        d_vocab=tokenizer.vocab_size,\n",
    "        n_heads=4,\n",
    "        n_layers=6,\n",
    "        d_mlp=512,\n",
    "        n_ctx=512,\n",
    "        act_fn=nn.GELU  # explicitly set to GELU\n",
    "    )\n",
    "\n",
    "    # Load some Gutenberg books\n",
    "    book_ids = [84, 85]\n",
    "    dataset_text = get_many_books(book_ids, data_temp=\"./data/gutenberg_data\")\n",
    "\n",
    "    rawtext = \"\"\n",
    "    for book in dataset_text:\n",
    "        rawtext += book\n",
    "\n",
    "    tokens = tokenizer(rawtext, return_tensors=\"pt\")\n",
    "\n",
    "    # Prepare data: split tokens into chunks\n",
    "    chunk_size = 100\n",
    "    to_remove = tokens[\"input_ids\"].shape[1] % chunk_size\n",
    "    new_shape = tokens[\"input_ids\"].shape[1] // chunk_size\n",
    "\n",
    "    if to_remove > 0:\n",
    "        attention_mask = tokens[\"attention_mask\"][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "        input_ids = tokens[\"input_ids\"][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "    else:\n",
    "        attention_mask = tokens[\"attention_mask\"][0].reshape(new_shape, chunk_size)\n",
    "        input_ids = tokens[\"input_ids\"][0].reshape(new_shape, chunk_size)\n",
    "\n",
    "    dataset = TensorDataset(input_ids, attention_mask)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    print(\"Input_ids shape:\", input_ids.shape)\n",
    "    print(\"Dataloader:\", dataloader)\n",
    "\n",
    "    # Initialize model and training\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Transformer(model_cfg).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "    n_epochs = 2\n",
    "    print_interval = 10\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for step, batch in enumerate(dataloader):\n",
    "            # batch[0]: [batch_size, seq_len]\n",
    "            input_ids_batch = batch[0].to(device)\n",
    "            total_loss = 0.0\n",
    "            bs = input_ids_batch.size(0)\n",
    "\n",
    "            # Process each sequence individually\n",
    "            for i in range(bs):\n",
    "                seq_ids = input_ids_batch[i]  # shape [seq_len]\n",
    "                # Next-token prediction: input is all but last token, target is all but first\n",
    "                inp = seq_ids[:-1]\n",
    "                targ = seq_ids[1:]\n",
    "\n",
    "                logits = model(inp)  # [seq_len-1, d_vocab]\n",
    "                loss = criterion(logits, targ)\n",
    "                total_loss += loss\n",
    "\n",
    "            total_loss = total_loss / bs\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (step + 1) % print_interval == 0:\n",
    "                print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {total_loss.item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
