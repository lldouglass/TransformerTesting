{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dataclasses import dataclass\n",
    "from jaxtyping import Float, Int\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_mask(n_context: int) -> Float[torch.Tensor, \"n_context n_context\"]:\n",
    "def create_mask(n_context: int) -> torch.Tensor:\n",
    "    mask = torch.zeros(n_context, n_context)\n",
    "    indices = torch.triu_indices(n_context, n_context, offset=1)\n",
    "    mask[indices[0], indices[1]] = float('-inf') \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configuration class for our transformer\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "\td_vocab: int = 10_000 \n",
    "\td_model: int = 128\n",
    "\td_mlp: int = 512\n",
    "\tn_heads: int = 4\n",
    "\td_head: int = 32\n",
    "\tn_layers: int = 6\n",
    "\tmax_ctx: int = 512\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\t# Define learned attention matrices\n",
    "\t\tself.W_Q = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_K = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\t\tself.W_O = nn.Linear(cfg.d_head, cfg.d_model)\n",
    "\t\tself.W_V = nn.Linear(cfg.d_model, cfg.d_head)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\tM = create_mask(x.size(0)).to(x.device) # Make sure mask is on same device as input tensor\n",
    "\t\treturn F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n",
    "\t\t\n",
    "\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# List of attention heads\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead(cfg) for i in range(cfg.n_heads)])\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_model\"]) -> Float[torch.Tensor, \"n_context d_model\"]:\n",
    "\t\t\n",
    "\t\t\n",
    "\t\t\n",
    "\t\thead_outputs = [head(x) for head in self.heads]  # List of tensors\n",
    "\t\tsum_output = sum(head_outputs) # Adds all head outputs together\n",
    "\n",
    "\t\treturn sum_output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Define layers of MLP\n",
    "\t\tself.Hidden = nn.Linear(cfg.d_model, cfg.d_mlp)\n",
    "\t\tself.Output = nn.Linear(cfg.d_mlp, cfg.d_model)\n",
    "\n",
    "\t\t# Using GELU activation function\n",
    "\t\tself.gelu = nn.GELU()\n",
    "\t\t\n",
    "\tdef forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "\t\t# It's an MLP\n",
    "\t\treturn self.gelu(self.Output(self.gelu(self.Hidden(x))))\n",
    "\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "\tdef __init__(self, cfg: GPTConfig):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.cfg = cfg\n",
    "\n",
    "\t\t# Creates positional embedding matrix that covers all possible context lengths\n",
    "\t\tself.pos_embedding = nn.Embedding(cfg.max_ctx, cfg.d_model)\n",
    "\n",
    "\t\t# Embedding and unembedding matrices\n",
    "\t\tself.embedding = nn.Embedding(cfg.d_vocab, cfg.d_model)\n",
    "\t\tself.unembedding = nn.Linear(cfg.d_model, cfg.d_vocab)\t\n",
    "\n",
    "\t\t# Layernorm\n",
    "\t\tself.norm = nn.LayerNorm(cfg.d_model)\n",
    "\n",
    "\t\t# Creates dictionary of attention heads and mlps depending on transformer depth\n",
    "\t\tself.layers = nn.ModuleList(\n",
    "\t\t\tnn.ModuleDict({\n",
    "\t\t\t\t'attn': MultiHeadedAttention(cfg),\n",
    "\t\t\t\t'mlp': MLP(cfg)\n",
    "\t\t\t}) for _ in range(cfg.n_layers)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: Float[torch.Tensor, \"n_context d_vocab\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "  \n",
    "\t\tx = self.embedding(x) # shape (n_context, d_model)\n",
    "\t\t\n",
    "\t\tpositions = torch.arange(x.size(0), device=x.device)  # (n_context, 1)\n",
    "\t\tpos_emb = self.pos_embedding(positions)  # (n_context, d_model)\n",
    "\n",
    "\t\tx = x + pos_emb\n",
    "\n",
    "\t\tfor layer in self.layers:\n",
    "\t\t\t\n",
    "\t\t\t# Residual connections for each layer\n",
    "\t\t\tx = x + layer['attn'](self.norm(x)) \n",
    "\t\t\tx = x + layer['mlp'](self.norm(x))\n",
    "\t\t\n",
    "\t\treturn self.unembedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = GPTConfig()\n",
    "#model = Transformer(cfg)\n",
    "n_context = 10\n",
    "x = torch.randint(0, cfg.d_vocab, (n_context,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10])\n",
      "torch.Size([10, 10000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiff\\AppData\\Local\\Temp\\ipykernel_24276\\543826298.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transform = Transformer(cfg)\n",
    "print(x.shape)\n",
    "output = transform(x)\n",
    "print(output.shape)\n",
    "# print(attn(x))\n",
    "# print(mh_attn(x))\n",
    "\n",
    "# visualize the attention scores\n",
    "plt.imshow(output.detach().numpy()[:, :100], cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label=\"Value\")  # Add a color bar\n",
    "plt.title(\"Attention Output Visualization\")\n",
    "plt.xlabel(\"Context\")\n",
    "plt.ylabel(\"Context\")\n",
    "plt.show()\n",
    "# visualize the attention scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting book 84...\n",
      "\t426785 characters read\n",
      "Getting book 84...\n",
      "\t359291 characters read\n",
      "Getting book 84...\n",
      "\t656545 characters read\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (385869 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3858, 100])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from get_books import get_many_books\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Initialize tokenizer once\n",
    "model_cfg = GPTConfig(d_vocab=tokenizer.vocab_size) # Configure the model for GPT2 vocab size\n",
    "\n",
    "# Import some books and pull them all into one giant string\n",
    "book_ids = [84, 85, 86]\n",
    "dataset = get_many_books(book_ids, data_temp=\"./data/gutenberg_data\")\n",
    "rawtext = \"\"\n",
    "for book in dataset:\n",
    "    rawtext += book\n",
    "\n",
    "# Tokenize the entire dataset (ignore warning about sequence length thrown here)\n",
    "tokens = tokenizer(rawtext, return_tensors=\"pt\")\n",
    "\n",
    "# Reorganize tokens into lengths of chunk_size\n",
    "chunk_size = 100\n",
    "to_remove = tokens[\"input_ids\"].shape[1] % chunk_size\n",
    "new_shape = tokens[\"input_ids\"].shape[1] // chunk_size\n",
    "attention_mask = tokens['attention_mask'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "input_ids = tokens['input_ids'][0][:-to_remove].reshape(new_shape, chunk_size)\n",
    "\n",
    "# Format tokens for dataloader and load them in\n",
    "tensor = TensorDataset(input_ids, attention_mask)\n",
    "dataloader = DataLoader(tensor, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiff\\AppData\\Local\\Temp\\ipykernel_24276\\543826298.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Step 10, Loss: 8.3131\n",
      "Epoch 1, Step 20, Loss: 6.9440\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[109]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     35\u001b[39m targ = seq_ids[\u001b[32m1\u001b[39m:]    \u001b[38;5;66;03m# shape [seq_len - 1]\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Your model returns logits of shape [seq_len-1, d_vocab]\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Compute loss across this sequence\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# CrossEntropyLoss expects [batch, vocab], so we can pass [seq_len-1, d_vocab] vs. [seq_len-1]\u001b[39;00m\n\u001b[32m     43\u001b[39m loss = criterion(logits, targ)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    102\u001b[39m x = x + pos_emb\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m    105\u001b[39m \t\n\u001b[32m    106\u001b[39m \t\u001b[38;5;66;03m# Residual connections for each layer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m \tx = x + \u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattn\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m    108\u001b[39m \tx = x + layer[\u001b[33m'\u001b[39m\u001b[33mmlp\u001b[39m\u001b[33m'\u001b[39m](\u001b[38;5;28mself\u001b[39m.norm(x))\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unembedding(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mMultiHeadedAttention.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn_context d_model\u001b[39m\u001b[33m\"\u001b[39m]) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn_context d_model\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m \thead_outputs = [\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m head \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.heads]  \u001b[38;5;66;03m# List of tensors\u001b[39;00m\n\u001b[32m     44\u001b[39m \tsum_output = \u001b[38;5;28msum\u001b[39m(head_outputs) \u001b[38;5;66;03m# Adds all head outputs together\u001b[39;00m\n\u001b[32m     46\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m sum_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1735\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1736\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\shiff\\Documents\\MATH598B\\TransformerTesting\\myenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1743\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1745\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1746\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1747\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1749\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1750\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mAttentionHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn_context d_model\u001b[39m\u001b[33m\"\u001b[39m]) -> Float[torch.Tensor, \u001b[33m\"\u001b[39m\u001b[33mn_context d_model\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \tM = \u001b[43mcreate_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.to(x.device) \u001b[38;5;66;03m# Make sure mask is on same device as input tensor\u001b[39;00m\n\u001b[32m     26\u001b[39m \t\u001b[38;5;28;01mreturn\u001b[39;00m F.softmax(\u001b[38;5;28mself\u001b[39m.W_Q(x) @ \u001b[38;5;28mself\u001b[39m.W_K(x).T + M) @ \u001b[38;5;28mself\u001b[39m.W_O(\u001b[38;5;28mself\u001b[39m.W_V(x))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcreate_mask\u001b[39m\u001b[34m(n_context)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_mask\u001b[39m(n_context: \u001b[38;5;28mint\u001b[39m) -> torch.Tensor:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     mask = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     indices = torch.triu_indices(n_context, n_context, offset=\u001b[32m1\u001b[39m)\n\u001b[32m      5\u001b[39m     mask[indices[\u001b[32m0\u001b[39m], indices[\u001b[32m1\u001b[39m]] = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m-inf\u001b[39m\u001b[33m'\u001b[39m) \n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Transformer(model_cfg).to(device)\n",
    "\n",
    "# Set up optimizer and loss\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Put model into training mode to store derivatives (important to do this after it is on gpu)\n",
    "model.train()\n",
    "\n",
    "\n",
    "n_epochs = 1\n",
    "print_interval = 10\n",
    "for epoch in range(n_epochs):\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        # [batch_size, seq_len]\n",
    "        input_ids_batch = batch[0].to(device)\n",
    "\n",
    "        # We'll accumulate the losses for each sequence in this mini-batch\n",
    "        total_loss = 0.0\n",
    "        batch_size = input_ids_batch.size(0)\n",
    "        \n",
    "        # Process each sequence individually\n",
    "        for i in range(batch_size):\n",
    "            # Extract a single sequence of shape [seq_len]\n",
    "            seq_ids = input_ids_batch[i]\n",
    "\n",
    "            # Next-token language modeling: input is all but last token, target is all but first\n",
    "            inp = seq_ids[:-1]   # shape [seq_len - 1]\n",
    "            targ = seq_ids[1:]    # shape [seq_len - 1]\n",
    "\n",
    "            # Forward pass\n",
    "            # Your model returns logits of shape [seq_len-1, d_vocab]\n",
    "            logits = model(inp)\n",
    "\n",
    "            # Compute loss across this sequence\n",
    "            # CrossEntropyLoss expects [batch, vocab], so we can pass [seq_len-1, d_vocab] vs. [seq_len-1]\n",
    "            loss = criterion(logits, targ)\n",
    "\n",
    "            # Accumulate\n",
    "            total_loss += loss\n",
    "\n",
    "        # Average across all sequences in the batch\n",
    "        total_loss = total_loss / batch_size\n",
    "\n",
    "        # Backprop and update\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print progress\n",
    "        if (step + 1) % print_interval == 0:\n",
    "            print(f\"Epoch {epoch+1}, Step {step+1}, Loss: {total_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shiff\\AppData\\Local\\Temp\\ipykernel_24276\\3943842642.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.softmax(self.W_Q(x) @ self.W_K(x).T + M) @ self.W_O(self.W_V(x)) # Attention equation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and standing that they were\n",
      "\n",
      " breeds H\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " CHAPTER XL THE LXXXIX.— Nay.—BOOK apologyonder AND verticalIXIII.— THEkin.—ThoseVI.—CHAPTER Locksley� tells too long nothing had just led with as\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " CHAPTERIII; the oneMat.—\n",
      "\n",
      " CHAPTER CHAPTER XXIX death.—The earushima, when young WH.— Somouth OF Mostnight.\n",
      "\n",
      " CHAPTER IV wasIX\n",
      "\n",
      "     IV"
     ]
    }
   ],
   "source": [
    "\n",
    "model.to('cpu')\n",
    "model.eval()\n",
    "\n",
    "output_tokens = 100\n",
    "\n",
    "test = \"More tokens please\"\n",
    "\n",
    "for new_token in range(output_tokens):\n",
    "    test_tokens = tokenizer(test, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = F.softmax(model(test_tokens['input_ids'][0]), dim=-1)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    samples = torch.multinomial(output, 1)\n",
    "\n",
    "\n",
    "   \n",
    "    detokenized_text = tokenizer.decode(samples[-1][0], skip_special_tokens=True)\n",
    "        \n",
    "    \n",
    "    print(detokenized_text,end='')\n",
    "\n",
    "    test += detokenized_text\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
